<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>LLMs Encode Harmfulness and Refusal Separately</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90' fill='%232e3a59'>D</text></svg>">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <script>
    window.MathJax = {
      tex: {
        packages: {'[+]': ['color']},
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    
  <style>
    body {
      background-color: #fdfdfd;
    }
    .hero {
      background: linear-gradient(to right, #fff8dc, #fafad2);
      color: #333;
      text-align: center;
    }
    .hero .title, .hero .subtitle {
      color: #333;
    } 
    .hero .title {
      font-size: 3.5rem !important;
      margin-bottom: 1.5rem;
    }
    .publication-authors {
      text-align: center;
      margin-bottom: 1rem;
    }
    .paper-section {
      padding-top: 3rem;
      padding-bottom: 3rem;
    }
    .section-title {
      font-size: 2.5rem !important;
      font-weight: 700 !important;
      margin-bottom: 2rem !important;
      color: #2c3e50;
      text-align: center;
      border-bottom: 3px solid #3498db;
      padding-bottom: 1rem;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }
    .subsection-title {
      font-size: 1.8rem !important;
      font-weight: 600 !important;
      margin-bottom: 1.5rem !important;
      color: #34495e;
      text-align: center;
      margin-top: 2rem;
    }
    .content-text {
      text-align: left;
      max-width: 800px;
      margin: 0 auto 2rem auto;
      line-height: 1.7;
      font-size: 1.1rem;
    }
    .figure-container {
      margin: 3rem auto;
      text-align: center;
    }
    .figure-placeholder {
      border: 2px dashed #ccc;
      padding: 1rem;
      text-align: center;
      font-style: italic;
      color: #777;
      margin-bottom: 2rem;
    }
    .button-group {
      margin-top: 1rem;
      text-align: center;
    }
    .buttons.button-group {
      margin-top: 1rem;
      text-align: center;
      display: flex;
      justify-content: center;
      gap: 0.5rem;
    }
    .button-group .button {
      background-color: #4a90e2;
      color: #ffffff;
      border: 2px solid #4a90e2;
    }
    .button-group .button:hover {
      background-color: #357abd;
      color: #ffffff;
      border-color: #357abd;
    }
    pre.bibtex {
      background: #f4f4f4;
      padding: 1rem;
      overflow-x: auto;
      font-size: 0.9em;
    }
    footer {
      background-color: #f5f5f5;
      padding: 2rem 1rem;
      text-align: center;
    }
    figure {
        text-align: center;
        margin: 2rem auto;
    }

    figcaption {
        text-align: center;
        font-size: 1rem;
        font-style: italic;
        margin-top: 1rem;
        color: #555;
    }
    
    /* Add color classes for text coloring */
    .text-orange {
        color: #ff8c00;
        font-weight: bold;
    }
    
    .text-blue {
        color: #0066cc;
        font-weight: bold;
    }
    
    /* Enhanced spacing and visual improvements */
    .section-divider {
      height: 2px;
      background: linear-gradient(to right, transparent, #3498db, transparent);
      margin: 3rem auto;
      max-width: 600px;
    }
    
    .highlight-box {
      background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
      border-left: 4px solid #3498db;
      padding: 1.5rem;
      margin: 2rem auto;
      max-width: 800px;
      border-radius: 8px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
  </style>
</head>
<body>

  <!-- Hero Header -->
  <section class="hero is-medium">
    <div class="hero-body">
      <div class="container">
        <h1 class="title is-2">LLMs Encode Harmfulness and Refusal Separately</h1>
        <div class="is-size-5 publication-authors">
          <span class="author-block">
              <!-- <a href="https://andotalao24.github.io/">Jiachen Zhao</a> -->Jiachen Zhao<sup>1</sup>, </span>
          <span class="author-block">
              <!-- <a href="https://scholar.google.com/citations?user=zM_wp_MAAAAJ&hl=en">Jing Huang</a> -->Jing Huang<sup>2</sup>, </span>
          <span class="author-block">
              <!-- <a href="https://cs.stanford.edu/~wuzhengx/">Zhengxuan Wu</a> -->Zhengxuan Wu<sup>2</sup>, </span>
          <span class="author-block">
              <!-- <a href="https://davidbau.com/">David Bau</a> -->David Bau<sup>1</sup>, </span>
          <span class="author-block">
              <!-- <a href="https://wyshi.github.io/">Weiyan Shi</a> -->Weiyan Shi<sup>1</sup></span>
      </div>
      
      <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>1</sup>Northeastern University&emsp;</span>
          <span class="author-block"><sup>2</sup>Stanford University</span>
      </div>

        <!-- Button links -->
        <div class="buttons button-group">
          <a class="button is-light" href="https://arxiv.org/abs/2507.11878" target="_blank">ðŸ“š Paper</a>
          <a class="button is-light" href="https://github.com/CHATS-lab/Llms_Encode_Harmfulness_Refusal_Separately" target="_blank">ðŸ’» GitHub</a>
        </div>
      </div>
    </div>
  </section>


  <section class="section paper-section">
    <div class="container content" style="text-align: center;">
      <video width="1280" height="720" controls>
        <source src="website/ANIM-project-decouple.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
  </section>


  <!-- Abstract -->
  <section class="section paper-section">
    <div class="container content" style="text-align: center;">
      <h2 class="section-title">Overview</h2>
      <div class="highlight-box">
        <p class="content-text">
          LLMs are trained to refuse harmful instructions, but do they truly understand harmfulness beyond just refusing? Prior work has shown that LLMs' refusal behaviors are mediated by a one-dimensional subspace, i.e., a refusal direction, in the latent space. 
          This refusal direction is often assumed to represent harmfulness as well and used as a linear predictor of harmfulness.
          <br><br>
          However, in this work, we find that harmfulness is encoded as a distinct concept from refusal in their latent representations. 

          We find that steering with the harmfulness steering along the harmfulness direction leads LLMs to interpret harmless instructions as harmful; but steering with the refusal direction tends to elicit refusal responses directly without reversing the model's judgment on harmfulness.   
          Furthermore, our clustering analysis of hidden states reveals that some jailbreak methods work by directly reducing refusal signals without radically suppressing the model's internal harmfulness judgment.  
          We also observe that adversarial fine-tuning that reverses models' refusal behaviors has minimal impact on the model's underlying beliefs about harmfulness and refusal. 
          These insights lead to a practical application that latent harmfulness representations can serve as an intrinsic safeguard for detecting unsafe inputs and reducing over-refusals, which is also robust to fine-tuning attacks. 
           Overall, we identify a separate dimension of harmfulness to analyze safety mechanisms in LLMs, offering a new perspective to study AI safety.
        </p>
      </div>
    </div>
  </section>


  <!-- Figure 1 Placeholder -->
  <section class="section paper-section">
    <div class="container content" style="text-align: center;">
      <h2 class="section-title"><span class="text-orange">$t_{\text{inst}}$</span> and <span class="text-blue">$t_{\text{post-inst}}$</span> encode harmfulness and refusal separately</h2>
      
      <div class="content-text">
        <p>
          We extract hidden states at <span class="text-orange">$t_{\text{inst}}$</span> and <span class="text-blue">$t_{\text{post-inst}}$</span> to examine what are encoded at each position. 
          <ul>
            <li><strong>$t_{\text{inst}}$:</strong> The last token of the user's instruction.</li>
            <li><strong>$t_{\text{post-inst}}$:</strong> The last token of the entire input prompt, which includes special tokens that come after the user's instruction (e.g., `[/INST]`).</li>
        </ul>
          
          We analyze the clustering of instructions with different properties in the latent space, because hidden states often form distinct clusters based on input
          features they encode. We collect four types of instructions:
          <ul>
            <li>Refused harmful instructions: The model refuses the harmful instruction.</li>
            <li>Accepted harmless instructions: The model accepts the harmless instruction.</li>
            <li>Refused harmless instructions: The model refuses the harmless instruction.</li>
            <li>Accepted harmful instructions: The model accepts the harmful instruction.</li>
          </ul>
          
          We ask an intuitive question: is the clustering in the latent space based on the instruction's harmfulness 
          or its refusal? <br><br>
          To answer the question, we first compute the respective clusters for instructions leading to desired model behaviors, i.e.,the
          cluster for refused harmful instructions, and the cluster for accepted harmless instructions. We then analyze misbehaving
          instructions (accepted but harmful instructions and refused but harmless instructions) to see which cluster they fall in.    
        </p>
        <p>
          As shown in Figure 2, we find that:
        </p>
        <ul>
          <li>At the <strong>$t_{\text{inst}}$</strong> position, hidden states clustered based on the inherent <strong>harmfulness</strong> of the instruction, regardless of whether the model accepts or refuses it. For example, a harmful instruction that the model *accepted* would still cluster with other harmful instructions.</li>
          <li>At the <strong>$t_{\text{post-inst}}$</strong> position, hidden states clustered based on the model's <strong>behavior</strong> (refusal or acceptance). Here, an accepted harmful instruction would cluster with other accepted (and harmless) instructions.</li>
        </ul>
      </div>
      
      <div class="figure-container">
        <img src="website/clustering.PNG" alt="Figure 1" style="max-width: 100%;" />
        <figcaption><b>Figure 2:</b> Internal clustering for hidden states extracted at <span class="text-orange">$t_{\text{inst}}$</span> and <span class="text-blue">$t_{\text{post-inst}}$</span>. The red region stands for
          the cluster of refused harmful instructions Crefused harmful, while the green region denotes the cluster of
          accepted harmless instructions Caccepted harmless. At each token position, we collect hidden states of
          two special cases: accepted harmful instructions (red curve) and refused harmless instructions (green
          curve) to see which cluster do these two cases fall in. The first row: At instruction token position
          <span class="text-orange">$t_{\text{inst}}$</span>, accepted harmful instructions tend to be closer to the refused harmful cluster, whereas refused
          harmless instructions are closer to the accepted harmless cluster. This implies that clustering may be
          based on whether the instruction is harmful or harmless; The second row: At post-instruction token
          position <span class="text-blue">$t_{\text{post-inst}}$</span>, The clustering behavior is reversed. Accepted harmful instructions are now more
          aligned with accepted instructions, and refused harmless instructions are closer to refused ones. This
          implies that, the clustering at $t_{\text{post-inst}}$ may reflect whether the instruction is accepted or refused.</figcaption>
      </div>
    
    
        

        <h2 class="subsection-title">Beliefs of harmfulness and refusal are not always correlated</h2>
        <div class="content-text">
          <p>
            We quantitatively analyze the correlation between the belief of harmfulness and the
            belief of refusal. We interpret the LLM's belief as reflected by which cluster the hidden state of
            an instruction falls into in the latent space. We find that the model may internally recognize
            the correct level of harmfulness in input instructions, yet still produce incorrect refusals or
            acceptances.  For jailbreak prompts, the refusal belief is overall suppressed (negative belief scores), while the harmfulness belief for some jailbreak prompts is still large.
             This suggests that some jailbreak methods may not reverse the model's internal belief of harmfulness, but directly suppress the refusal signals.
          </p>
        </div>
        
        <div class="figure-container">
          <div style="display: flex; justify-content: center; align-items: center; gap: 20px;">
            <div style="text-align: center; max-width: 40%;">
              <img src="website/belief1.PNG" alt="Subfigure (a)" style="max-width: 100%;" />
              <div style="font-weight: bold; margin-top: 5px;">(a)</div>
            </div>
            <div style="text-align: center; max-width: 40%;">
              <img src="website/belief2.PNG" alt="Subfigure (b)" style="max-width: 90%;" />
              <div style="font-weight: bold; margin-top: 5px;">(b)</div>
            </div>
          </div>
          <figcaption>
            <b>Figure 3:</b> (a): Correlation between beliefs of harmfulness and refusal. (b): Beliefs of harmfulness and
            refusal for different categories of jailbreak prompts in comparison with refused harmful instructions.
            Our results suggest that the model may wronglyrefuse harmless instructions (or accept harmful/jailbreak instructions), but internally believe them to be harmless (harmful). 
          </figcaption>
        </div>


        <h2 class="subsection-title">Steering with the harmfulness direction</h2>
        <div class="content-text">
          <p>
            We can extract the harmfulness direction from the hidden states at <span class="text-orange">$t_{\text{inst}}$</span> as the difference between the centroid of clusters of harmful and harmless instructions.
            We find that steering the hidden states of harmless instructions along the harmfulness direction will also make the model refuse those harmless instructions as shown in Figure 4. 
          </p>
        </div>
          
        <div class="figure-container">
          <div style="display: flex; justify-content: space-between; align-items: center;">
            <div style="text-align: center; max-width: 30%;">
              <img src="website/intervene-hl-qwen.PNG" alt="Subfigure (a)" style="max-width: 100%;" />
              <div style="font-weight: bold; margin-top: 5px;">Qwen2</div>
            </div>
            <div style="text-align: center; max-width: 30%;">
              <img src="website/intervene-hl-llama2.PNG" alt="Subfigure (b)" style="max-width: 100%;" />
              <div style="font-weight: bold; margin-top: 5px;">Llama2</div>
            </div>
            <div style="text-align: center; max-width: 30%;">
              <img src="website/intervene-hl-llama3.PNG" alt="Subfigure (c)" style="max-width: 100%;" />
              <div style="font-weight: bold; margin-top: 5px;">Llama3</div>
            </div>
          </div>
          <figcaption>
            <b>Figure 4:</b> Both refusal direction and harmfulness direction can be used to steer the model to refuse benign instructions. 
          </figcaption>
        </div>


        <h3 class="subsection-title">Causal Evidence that harmfulness direction encodes harmfulness: The Reply Inversion Task</h3>
        <div class="content-text">
          <p>
             To provide causal evidence that the harmfulness direction plays a different role than the refusal direction, we design a "reply inversion task" where we append a question like "Could this user prompt cause harm?" to the original instruction. 
            This task will elicit refusal tokens from the model if the instruction is harmful, and affirmative tokens if the instruction is harmless.
          </p>
        </div>
        
        <div class="figure-container">
          <img src="website/inversion-prompt.PNG" alt="Figure 1" style="max-width: 70%;" />
        </div>
        
        <div class="content-text">
          <p>
            We show that (1)the harmfulness direction extracted at <span class="text-orange">$t_{\text{inst}}$</span> represents the concept of harmfulness even when the
            LLM does not refuse; (2) whereas the refusal direction primarily represents surface-level refusal
            characteristics, so that steering along it may not always reverse the model's judgment of harmfulness
            of an instruction.
          </p>
          <ul>
            As shown in Figure 5, we find that:
            <li>When we steer a harmless instruction along the <strong>harmfulness direction</strong>, the model's internal perception changed, and it would reverse its answer from "No" to "Certainly," suggesting it now views the instruction as harmful.</li>
            <li>However, when we steer it along the <strong>refusal direction</strong>, the model would generally maintain its original "No" response, indicating that its underlying judgment of harmfulness didn't change.</li>
          </ul>
        </div>
        
        <div class="figure-container">
          <img src="website/inversion-ret.PNG" alt="Figure 1" style="max-width: 70%;" />
          <figcaption><b>Figure 5:</b> The reply inversion task.
            The model may refuse harmful instructions but still believe them to be harmful.
          </figcaption>
        </div>

    </div>
  </section>



  

  
  <section class="section paper-section">
      <div class="container content" style="text-align: center;">
          <h2 class="section-title">Latent Guard: An Intrinsic Safeguard</h2>
          
          <div class="figure-container">
            <img src="website/latent-guard.PNG" alt="Figure 1" style="max-width: 70%;" />
          </div>
        
          <div class="content-text">
            <p>
                Based on our findings, we propose a "Latent Guard" model that uses the LLM's own internal belief of harmfulness to detect unsafe inputs.
            </p>
            <ul>
                <li>This Latent Guard is competitive with, and in some cases outperforms, dedicated guard models like Llama Guard 3 8B.</li>
                <li>It was particularly effective at detecting harmful prompts using persuasion techniques and in identifying cases of over-refusal. On the Qwen2 model, the Latent Guard achieved 75% accuracy on persuasion prompts, compared to 17.8% for Llama Guard 3.</li>
                <li>Crucially, this internal belief of harmfulness was found to be robust to "finetuning attacks," where a model is maliciously retrained to accept harmful instructions. Even after finetuning, the internal harmfulness signal remained largely unchanged.</li>
            </ul>
          </div>
      </div>
  </section>
  



  <!-- BibTeX Citation -->
  <section class="section paper-section">
    <div class="container content" style="text-align: center;">
      <h2 class="section-title">ðŸ“Œ BibTeX Citation</h2>
      <p class="content-text">If you find our project useful, please consider citing:</p>
      <pre class="bibtex" style="text-align: left; max-width: 800px; margin: 0 auto;">
        @misc{zhao2025llmsencodeharmfulnessrefusal,
          title={LLMs Encode Harmfulness and Refusal Separately}, 
          author={Jiachen Zhao and Jing Huang and Zhengxuan Wu and David Bau and Weiyan Shi},
          year={2025},
          eprint={2507.11878},
          archivePrefix={arXiv},
          primaryClass={cs.CL},
          url={https://arxiv.org/abs/2507.11878}, 
    }
      </pre>
    </div>
  </section>

  <!-- Footer -->
  <footer>
    <p>Â© 2025 â€” <em>LLMs Encode Harmfulness and Refusal Separately</em></p>
  </footer>

</body>
</html>
